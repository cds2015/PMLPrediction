---

title: "Practical machine learning Assignment"

output: html_document

---



Load Dataset



```{r, chunk1}



library(caret)



library(rpart)

library(rpart.plot)



library(RColorBrewer)

library(RGtk2)

library(rattle)

library(randomForest)



library(gbm)



train_url <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

test_url  <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"





init_org_training_data <- read.csv(url(train_url))

init_org_testing_data <- read.csv(url(test_url))



dim(init_org_training_data)

dim(init_org_testing_data)

```

Data Cleaning



```{r, chunk2}

non_zero_var <- nearZeroVar(init_org_training_data)





org_training_data <- init_org_training_data[,-non_zero_var]

org_testing_data <- init_org_testing_data[,-non_zero_var]



dim(org_training_data)

dim(org_testing_data)

na_val_col <- sapply(org_training_data, function(x) mean(is.na(x))) > 0.95



org_training_data <- org_training_data[,na_val_col == FALSE]

org_testing_data <- org_testing_data[,na_val_col == FALSE]



dim(org_training_data)

dim(org_testing_data)



org_training_data <- org_training_data[,8:59]

org_testing_data <- org_testing_data[,8:59]



dim(org_training_data)



dim(org_testing_data)



colnames(org_training_data)

colnames(org_testing_data)


```


Data Partitioning



```{r, chunk3}



inTrain <- createDataPartition(org_training_data$classe, p=0.6, list=FALSE)

training <- org_training_data[inTrain,]

testing <- org_training_data[-inTrain,]



dim(training)



dim(testing)
```


Decision Tree



```{r, chunk4}



DT_modfit <- train(classe ~ ., data = training, method="rpart")



DT_prediction <- predict(DT_modfit, testing)

confusionMatrix(DT_prediction, testing$classe)





rpart.plot(DT_modfit$finalModel, roundint=FALSE)
```
As we can see the DT model has 60% accuracy. Let's try random forest model.



Random Forest Model



```{r,chunk5}

RF_modfit <- train(classe ~ ., data = training, method = "rf", ntree = 5)



RF_prediction <- predict(RF_modfit, testing)

RF_pred_conf <- confusionMatrix(RF_prediction, testing$classe)

RF_pred_conf

plot(RF_pred_conf$table, col = RF_pred_conf$byClass, 

     main = paste("Random Forest - Accuracy Level =",

                  round(RF_pred_conf$overall['Accuracy'], 4)))

```
Random Forest Model gave us 97% accuracy. Let's try Gradient Boosting Model.



GBM

```{r,chunk6}



GBM_modfit <- train(classe ~ ., data = training, method = "gbm", verbose = FALSE)

GBM_modfit$finalModel



GBM_prediction <- predict(GBM_modfit, testing)



GBM_pred_conf <- confusionMatrix(GBM_prediction, testing$classe)

GBM_pred_conf



plot(GBM_pred_conf$table, col = GBM_pred_conf$byClass, 

     main = paste("Gradient Boosting - Accuracy Level =",

                  round(GBM_pred_conf$overall['Accuracy'], 4)))



RF_pred_conf$overall



GBM_pred_conf$overall



Final_RF_prediction <- predict(RF_modfit, org_testing_data )

Final_RF_prediction
```

### Conclusion

As we can see GBM has 95% accuracy hence we go for random forest model for prediction.