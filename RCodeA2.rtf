{\rtf1\ansi\ansicpg1252\cocoartf2511
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier-Bold;\f1\fmodern\fcharset0 Courier;\f2\fnil\fcharset0 HelveticaNeue-Medium;
\f3\fmodern\fcharset0 Courier-Oblique;\f4\fnil\fcharset0 HelveticaNeue;\f5\fnil\fcharset0 HelveticaNeue-Bold;
}
{\colortbl;\red255\green255\blue255;\red133\green0\blue2;\red242\green242\blue242;\red38\green38\blue38;
\red255\green255\blue255;\red210\green0\blue53;\red135\green136\blue117;\red17\green137\blue135;\red133\green0\blue96;
}
{\*\expandedcolortbl;;\cssrgb\c60000\c0\c0;\cssrgb\c96078\c96078\c96078;\cssrgb\c20000\c20000\c20000;
\cssrgb\c100000\c100000\c100000;\cssrgb\c86667\c6667\c26667;\cssrgb\c60000\c60000\c53333;\cssrgb\c0\c60000\c60000;\cssrgb\c60000\c0\c45098;
}
\margl1440\margr1440\vieww25400\viewh16000\viewkind0
\deftab720
\pard\pardeftab720\sl360\partightenfactor0

\f0\b\fs26 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 library
\f1\b0 \cf4 \strokec4 (knitr)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (caret)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (rpart)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (rpart.plot)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (rattle)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (randomForest)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (RColorBrewer)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (RGtk2)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f0\b \cf2 \cb3 \strokec2 library
\f1\b0 \cf4 \strokec4 (gbm)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl660\sa200\partightenfactor0

\f2\fs60 \cf4 Loading Data\cb1 \
\pard\pardeftab720\sl360\partightenfactor0

\f1\fs26 \cf4 \cb3 train_url<- \cf6 \strokec6 "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"\cf4 \strokec4 \
test_url<- \cf6 \strokec6 "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"\cf4 \strokec4 \
training_data<- read.csv(url(train_url))\
testing_data<- read.csv(url(test_url))\
dim(training_data)\
dim(testing_data)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl660\sa200\partightenfactor0

\f2\fs60 \cf4 Data Cleansing\cb1 \
\pard\pardeftab720\sl360\partightenfactor0

\f3\i\fs26 \cf7 \cb3 \strokec7 ###Removing Variables which have Nearly Zero Variance.
\f1\i0 \cf4 \strokec4 \
nzv <- nearZeroVar(training_data)\
\
train_data <- training_data[,-nzv]\
test_data <- testing_data[,-nzv]\
\
dim(train_data)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3 dim(test_data)   \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f3\i \cf7 \cb3 \strokec7 ###Removing NA Values of Variables.  
\f1\i0 \cf4 \strokec4 \
na_val_col <- sapply(train_data, 
\f0\b \cf2 \strokec2 function
\f1\b0 \cf4 \strokec4 (x) mean(is.na(x))) > \cf8 \strokec8 0.95\cf4 \strokec4 \
train_data <- train_data[,na_val_col == \cf9 \strokec9 FALSE\cf4 \strokec4 ]\
test_data <- test_data[,na_val_col == \cf9 \strokec9 FALSE\cf4 \strokec4 ]\
\
dim(train_data)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3 dim(test_data)  \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f3\i \cf7 \cb3 \strokec7 ###Removing the first 7 Variables which are Non-Numeric.  
\f1\i0 \cf4 \strokec4 \
train_data<- train_data[, \cf8 \strokec8 8\cf4 \strokec4 :\cf8 \strokec8 59\cf4 \strokec4 ]\
test_data<- test_data[, \cf8 \strokec8 8\cf4 \strokec4 :\cf8 \strokec8 59\cf4 \strokec4 ]\
dim(train_data)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3 dim(test_data)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl660\sa200\partightenfactor0

\f2\fs60 \cf4 Data Partioning\cb1 \
\pard\pardeftab720\sl320\sa200\partightenfactor0

\f4\fs28 \cf4 \cb5 In this we will seggregate our\'a0
\f5\b train_data
\f4\b0 \'a0in two parts \'93
\f5\b training
\f4\b0 \'94(60% of data) and \'93
\f5\b testing
\f4\b0 \'94(40% of data)/ Validateion set.\cb1 \
\pard\pardeftab720\sl360\partightenfactor0

\f1\fs26 \cf4 \cb3 inTrain<- createDataPartition(train_data$classe, p=\cf8 \strokec8 0.6\cf4 \strokec4 , list=\cf9 \strokec9 FALSE\cf4 \strokec4 )\
inTrain<- createDataPartition(train_data$classe, p=\cf8 \strokec8 0.6\cf4 \strokec4 , list=\cf9 \strokec9 FALSE\cf4 \strokec4 )\
training<- train_data[inTrain,]\
testing<- train_data[-inTrain,]\
dim(training)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3 dim(testing)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\
\pard\pardeftab720\sl360\partightenfactor0

\f3\i \cf7 \cb3 \strokec7 ###Fit the model and plot   
\f1\i0 \cf4 \strokec4 \

\f0\b \cf2 \strokec2 library
\f1\b0 \cf4 \strokec4 (rattle)\
DT_model<- train(classe ~. , data=training, method= \cf6 \strokec6 "rpart"\cf4 \strokec4 )\
fancyRpartPlot(DT_model$finalModel)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f3\i \cf7 \cb3 \strokec7 ###Prediction   
\f1\i0 \cf4 \strokec4 \
set.seed(\cf8 \strokec8 21243\cf4 \strokec4 )\
DT_prediction<- predict(DT_model, testing)\
confusionMatrix(DT_prediction, testing$classe)\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl520\sa200\partightenfactor0

\f2\fs48 \cf4 Random Forest Model and Prediction\
\pard\pardeftab720\sl360\partightenfactor0

\f1\fs26 \cf4 \cb3 set.seed(\cf8 \strokec8 26817\cf4 \strokec4 )\

\f3\i \cf7 \strokec7 ###Fit the model   
\f1\i0 \cf4 \strokec4 \
RF_model<- train(classe ~. , data=training, method= \cf6 \strokec6 "rf"\cf4 \strokec4 , ntree=\cf8 \strokec8 100\cf4 \strokec4 )\

\f3\i \cf7 \strokec7 ###Prediction  
\f1\i0 \cf4 \strokec4 \
RF_prediction<- predict(RF_model, testing)\
RF_cm<-confusionMatrix(RF_prediction, testing$classe)\
RF_cm\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f3\i \cf7 \cb3 \strokec7 ###plot    
\f1\i0 \cf4 \strokec4 \
plot(RF_cm$table, col=RF_cm$byClass, main=\cf6 \strokec6 "Random Forest Accuracy"\cf4 \strokec4 )\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\
\pard\pardeftab720\sl520\sa200\partightenfactor0

\f2\fs48 \cf4 Gradient Boosting Model and Prediction\
\pard\pardeftab720\sl360\partightenfactor0

\f1\fs26 \cf4 \cb3 set.seed(\cf8 \strokec8 25621\cf4 \strokec4 )\
gbm_model<- train(classe~., data=training, method=\cf6 \strokec6 "gbm"\cf4 \strokec4 , verbose= \cf9 \strokec9 FALSE\cf4 \strokec4 )\
gbm_model$finalmodel\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0

\f3\i \cf7 \cb3 \strokec7 ###Prediction    
\f1\i0 \cf4 \strokec4 \
\
gbm_prediction<- predict(gbm_model, testing)\
gbm_cm<-confusionMatrix(gbm_prediction, testing$classe)\
gbm_cm\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl320\sa200\partightenfactor0

\f4\fs28 \cf4 From the\'a0
\f5\b Gradient Boosting Model
\f4\b0 \'a0we see the prediction accuracy is\'a0
\f5\b 96%
\f4\b0 \'a0which is satisfied.\
\pard\pardeftab720\sl360\partightenfactor0

\f3\i\fs26 \cf7 \cb3 \strokec7 ##we have taken Random Forest and Gradient Boosting Model because it reach to satisfied prediction level. we are compairing the both model which is more accurate.    
\f1\i0 \cf4 \strokec4 \
RF_cm$overall\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb3 gbm_cm$overall\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
\
\pard\pardeftab720\sl660\sa200\partightenfactor0

\f2\fs60 \cf4 Conclusion\cb1 \
\pard\pardeftab720\sl320\sa200\partightenfactor0

\f4\fs28 \cf4 \cb5 we conclude that,\'a0
\f5\b Random Forest
\f4\b0 \'a0is more accurate than Gradient Boosting Model at upto 99% of accuracy level.\cb1 \
\pard\pardeftab720\sl660\sa200\partightenfactor0

\f2\fs60 \cf4 \cb5 Prediction - using Random Forest MOdel on testing data.\cb1 \
\pard\pardeftab720\sl360\partightenfactor0

\f1\fs26 \cf4 \cb3 prediction_test<- predict(RF_model, test_data)\
prediction_test\
\pard\pardeftab720\sl360\partightenfactor0
\cf4 \cb5 \
}